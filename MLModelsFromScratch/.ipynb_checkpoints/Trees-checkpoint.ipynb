{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43df0a7-8c1a-4ca7-baea-500b18e1962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d9a31-296e-42ab-ae54-716b7eb1e437",
   "metadata": {},
   "source": [
    "### Decision Tree \n",
    "Init, gini coeffient,entropy,MSE, find the best fit[ best threshold and best feature],fir for all, predict single and then apply for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd00e69-7e0a-4815-ae3b-83a5faebd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, threshold=None, feature_idx=None, value=None, left=None, right=None, impurity_decrease = None):\n",
    "        ## create node\n",
    "        self.threshold = threshold\n",
    "        self.feature_idx = feature_idx\n",
    "        self.value = value \n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.impurity_decrease = impurity_decrease\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "    def gini(self, class_count,n):\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity for a list of class labels.\n",
    "        \"\"\"\n",
    "        gini_value = 1\n",
    "        for count in class_count.values():\n",
    "            prob = count / n\n",
    "            gini_value -= prob ** 2\n",
    "        return gini_value\n",
    "\n",
    "    def entropy(self, class_count,n):\n",
    "        \"\"\"\n",
    "        Calculate the entropy gain for a list of class labels.\n",
    "        \"\"\"\n",
    "\n",
    "        entropy_value = 0\n",
    "        for count in class_count.values():\n",
    "            prob = count / n\n",
    "            if prob>0:\n",
    "                entropy_value -= prob * np.log2(prob)\n",
    "                \n",
    "        return entropy_value\n",
    "\n",
    "    def mse(self, sum_y,sum_y2,n ):\n",
    "        \"\"\"\n",
    "        Calculate the mse for a list of regression labels.\n",
    "        \"\"\"\n",
    "        return (sum_y2/n)-(sum_y/n)**2\n",
    "    \n",
    "    def errorloss(self, X, y, feature_idx, task_type, criterion='gini', y_pred=None, loss=None, lambda_=1.0, gamma=0.0):\n",
    "        \"\"\"\n",
    "        Calculate error loss which gives best score and threshold for only one feature \n",
    "        \"\"\"\n",
    "        \n",
    "        ## sort the x's such that easy to calculate thresholds \n",
    "        # pairs = list(zip(X[:, feature_idx], y))\n",
    "        # sorted_pairs = sorted(pairs, key=lambda pair: pair[0])\n",
    "        # x_sorted, y_sorted = zip(*sorted_pairs)\n",
    "        \n",
    "        sorted_idx = np.argsort(X[:, feature_idx])\n",
    "        x_sorted = X[sorted_idx, feature_idx]\n",
    "        y_sorted = y[sorted_idx]\n",
    "        if task_type == 'gradient':\n",
    "            y_pred_sorted = y_pred[sorted_idx]\n",
    "\n",
    "\n",
    "        \n",
    "        # convert to array for better perfomance\n",
    "        x_sorted = np.array(x_sorted)\n",
    "        y_sorted = np.array(y_sorted)\n",
    "        \n",
    "        n = len(y_sorted)\n",
    "        \n",
    "        # create dummys which minimises the score and helps gets the better threshold \n",
    "        best_fit = -np.inf\n",
    "        best_threshold = 0\n",
    "        \n",
    "        \n",
    "        if task_type == 'class':\n",
    "            \n",
    "            ## create dict such that timecomplexity is reduced \n",
    "            total_counts = defaultdict(int)\n",
    "            \n",
    "            for i in y_sorted:\n",
    "                total_counts[i] +=1\n",
    "            total_counts_main = total_counts.copy()\n",
    "            left_counts = defaultdict(int)\n",
    "            \n",
    "            for i in range(1,n-1):\n",
    "                \n",
    "                left_value = y_sorted[i-1]\n",
    "                left_counts[left_value] +=1\n",
    "                total_counts[left_value] -=1\n",
    "                right_counts = total_counts\n",
    "                \n",
    "                left_n  = i\n",
    "                right_n = n-1-i\n",
    "                \n",
    "                threshold = (x_sorted[i-1] + x_sorted[i]) / 2\n",
    "\n",
    "                if left_n == 0 or right_n == 0:\n",
    "                    continue \n",
    "                    \n",
    "                if x_sorted[i-1] == x_sorted[i] :\n",
    "                    continue\n",
    "\n",
    "                if criterion == 'entropy':\n",
    "                    parent_loss = self.entropy(total_counts_main,n)\n",
    "                    left_loss = self.entropy(left_counts,left_n)\n",
    "                    right_loss = self.entropy(right_counts,right_n)\n",
    "                    error_loss = parent_loss - (left_n * left_loss + right_n * right_loss)\n",
    "                    if error_loss>best_fit:\n",
    "                        best_fit = error_loss\n",
    "                        best_threshold = threshold\n",
    "                    \n",
    "                elif criterion == 'gini':\n",
    "                    parent_loss = self.gini(total_counts_main,n)\n",
    "                    left_loss = self.gini(left_counts,left_n)\n",
    "                    right_loss = self.gini(right_counts,right_n)\n",
    "                    error_loss = parent_loss - (left_n * left_loss + right_n * right_loss)\n",
    "                    if error_loss>best_fit:\n",
    "                        best_fit = error_loss\n",
    "                        best_threshold = threshold\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid classicfication criteria.\")\n",
    "                \n",
    "        elif task_type == 'regression':\n",
    "            sum_total = np.sum(y_sorted)\n",
    "            sum_sq_total = np.sum(np.square(y_sorted))\n",
    "            sum_left = 0.0\n",
    "            sum_sq_left = 0.0\n",
    "            for i in range(1,n-1):\n",
    "                y_val = y_sorted[i-1]\n",
    "                sum_left += y_val\n",
    "                sum_sq_left += np.sum(np.square(y_val))\n",
    "                \n",
    "                sum_right = sum_total - sum_left\n",
    "                sum_sq_right = sum_sq_total - sum_sq_left\n",
    "                \n",
    "                left_n = i\n",
    "                right_n = n-1-i\n",
    "                \n",
    "                parent_loss = self.mse(sum_total,sum_sq_total,n)\n",
    "                left_loss = self.mse(sum_left,sum_sq_left,left_n)\n",
    "                right_loss = self.mse(sum_right,sum_sq_right,right_n)\n",
    "                error_loss = parent_loss - (left_n * left_loss + right_n * right_loss)\n",
    "                if error_loss>best_fit:\n",
    "                    best_fit = error_loss\n",
    "                    best_threshold = threshold\n",
    "        elif task_type == 'gradient':\n",
    "            g_sorted = loss.gradient(y_sorted,y_pred_sorted)\n",
    "            h_sorted = loss.hessian(y_sorted,y_pred_sorted)\n",
    "            g_total = sum(g_sorted)\n",
    "            h_total = sum(h_sorted)\n",
    "            g_left = 0\n",
    "            h_left = 0\n",
    "            for i in range(1,n-1):\n",
    "                g_left += g_sorted[i-1]\n",
    "                h_left += h_sorted[i-1]\n",
    "                g_right = g_total - g_left\n",
    "                h_right = h_total - h_left\n",
    "                threshold = (x_sorted[i-1] + x_sorted[i]) / 2\n",
    "                \n",
    "                if x_sorted[i-1] == x_sorted[i]:\n",
    "                    continue\n",
    "                gain = 0.5 * (\n",
    "                                (g_left ** 2) / (h_left + lambda_) +\n",
    "                                (g_right ** 2) / (h_right + lambda_) -\n",
    "                                (g_total ** 2) / (h_total + lambda_)\n",
    "                            ) - gamma\n",
    "                if gain>best_fit:\n",
    "                    best_fit = gain\n",
    "                    best_threshold = threshold\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task type.\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        # if error_loss>best_fit:\n",
    "        #     best_fit = error_loss\n",
    "        #     best_threshold = threshold\n",
    "\n",
    "        return best_threshold,best_fit\n",
    "    \n",
    "    def find_best_fit(self,X,y, task_type, criterion, max_features,y_pred=None, loss=None, lambda_=1.0, gamma=0.0):\n",
    "        \"\"\"\n",
    "        Calculate best threshold for each feature and get best feature in that fit \n",
    "        \"\"\"\n",
    "        best_fit = -np.inf\n",
    "        best_feature_idx = None\n",
    "        best_threshold = 0\n",
    "        \n",
    "        max_features = min(max_features, X.shape[1])\n",
    "        \n",
    "        if max_features is None:\n",
    "            max_features = int(np.sqrt(X.shape[1])) if task_type == 'class' else X.shape[1] // 3\n",
    "        feature_indices = np.random.choice(X.shape[1], size=max_features, replace=False)\n",
    "\n",
    "                    \n",
    "        for i in feature_indices:\n",
    "            if task_type!='gradient':\n",
    "                threshold,score =  self.errorloss(X,y,i, task_type, criterion)\n",
    "            else :\n",
    "                threshold,score =  self.errorloss(X,y,i, task_type, criterion,y_pred, loss, lambda_, gamma)\n",
    "                \n",
    "            if score>best_fit:\n",
    "                best_fit = score\n",
    "                best_feature_idx = i \n",
    "                best_threshold = threshold\n",
    "        return best_feature_idx,best_threshold,best_fit\n",
    "    \n",
    "    def majority_class(self,y,task_type):\n",
    "        \"\"\"\n",
    "        When stop condition hits return the value corresponding to task type \n",
    "        \"\"\"\n",
    "        \n",
    "        if task_type == 'class':\n",
    "            values,counts = np.unique(y, return_counts = True)\n",
    "            return  values[np.argmax(counts)]\n",
    "        elif task_type == 'regression':\n",
    "            return np.mean(y)\n",
    "        \n",
    "    \n",
    "    def fit(self,X,y,depth = 0,max_depth = 5,min_samples_split=2,task_type='class', criterion='gini', max_features = 3, y_pred=None, loss=None, lambda_=1.0, gamma=0.0):\n",
    "        \"\"\"\n",
    "        recursive function fit which creats the tree and nodes based on the best threshold and best feature \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        ## first stop condition if all y are same then stop which is puritycheck\n",
    "        if len(set(y))== 1:\n",
    "            self.value = y[0]\n",
    "            return\n",
    "        \n",
    "        ## second stop condition , dont go beyond  max_depth \n",
    "        if depth >= max_depth:\n",
    "            self.value = self.majority_class(y,task_type)\n",
    "            return\n",
    "        \n",
    "        ## if the split has sample size less than minum minimum sample split then stop\n",
    "        if len(y) < min_samples_split:\n",
    "            self.value = self.majority_class(y,task_type)\n",
    "            return \n",
    "        \n",
    "        ## get the best feature and threshold from find best fit \n",
    "        if task_type!='gradient':\n",
    "            best_feature_idx,best_threshold,best_fit = self.find_best_fit(X,y,task_type, criterion,max_features)\n",
    "        else :\n",
    "            best_feature_idx,best_threshold,best_fit = self.find_best_fit(X,y,task_type, criterion,max_features,y_pred, loss, lambda_, gamma)\n",
    "        \n",
    "        ## stop confition when there are no best features \n",
    "        if best_feature_idx is None:\n",
    "            self.value = self.majority_class(y, task_type)\n",
    "            return\n",
    "        \n",
    "        ## save asplit info for prediction \n",
    "        \n",
    "        self.threshold = best_threshold\n",
    "        self.feature_idx = best_feature_idx\n",
    "        self.impurity_decrease = best_fit\n",
    "        \n",
    "        \n",
    "        ## create next child nodes \n",
    "        print(f\"Depth: {depth}, Feature: {best_feature_idx}, Threshold: {best_threshold}\")\n",
    "        \n",
    "        mask_left = X[:, best_feature_idx] <= best_threshold\n",
    "        X_left = X[mask_left]\n",
    "        y_left = y[mask_left]\n",
    "        self.left = Node()\n",
    "        if task_type!='gradient':\n",
    "            self.left.fit(X_left,y_left,depth+1,max_depth,min_samples_split,task_type, criterion,max_features)\n",
    "        else :\n",
    "            self.left.fit(X_left,y_left,depth+1,max_depth,min_samples_split,task_type, criterion,max_features,y_pred, loss, lambda_, gamma)\n",
    "        \n",
    "        mask_right = X[:, best_feature_idx] > best_threshold\n",
    "        X_right = X[mask_right]\n",
    "        y_right = y[mask_right]\n",
    "        \n",
    "        self.right = Node()\n",
    "        # self.right.fit(X_right,y_right,depth+1,max_depth,min_samples_split,task_type, criterion,max_features)\n",
    "        if task_type!='gradient':\n",
    "            self.right.fit(X_right,y_right,depth+1,max_depth,min_samples_split,task_type, criterion,max_features)\n",
    "        else :\n",
    "            self.right.fit(X_right,y_right,depth+1,max_depth,min_samples_split,task_type, criterion,max_features,y_pred, loss, lambda_, gamma)\n",
    "        \n",
    "    def predict_single(self, x):\n",
    "        if self.is_leaf_node():\n",
    "            return self.value if self.value is not None else 0.0\n",
    "\n",
    "        if self.feature_idx is None:\n",
    "            return self.value if self.value is not None else 0.0\n",
    "\n",
    "        if x[self.feature_idx] <= self.threshold:\n",
    "            return self.left.predict_single(x) if self.left else (self.value if self.value is not None else 0.0)\n",
    "        else:\n",
    "            return self.right.predict_single(x) if self.right else (self.value if self.value is not None else 0.0)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        return [self.predict_single(row) for row in x.tolist()]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1ec04-1f1f-4467-b423-32bb541f97d1",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "bagged trees, call from decision tree nodes. Fit son random indices create multiple trees. keep storing best fit and finally give avergae of best fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7a173-6733-421e-a32f-a103401e1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTrees(object):\n",
    "    def __init__(self, n_estimators = 10, max_depth = 5, min_samples_split = 2, task_type = 'class', criterion = 'gini', max_features = 3, random_state = 3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.task_type = task_type\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self,X,y) : \n",
    "        for i in range(0, self.n_estimators):\n",
    "            np.random.seed(self.random_state + i)\n",
    "            indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "            \n",
    "            if self.max_features > X_sample.shape[1]:\n",
    "                max_features = X_sample.shape[1]\n",
    "            else : \n",
    "                max_features = self.max_features\n",
    "                \n",
    "            tree = Node()\n",
    "            tree.fit(X_sample,y_sample, depth = 0 ,max_depth = self.max_depth, min_samples_split = self.min_samples_split, task_type = self.task_type, criterion = self.criterion, max_features = max_features )\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict_single(self,x):\n",
    "        \n",
    "        if self.task_type == 'class':\n",
    "            predictions = 0\n",
    "            for i in self.trees:\n",
    "                predictions_value = i.predict_single(x)\n",
    "                if predictions_value == 1 :\n",
    "                    predictions += 1\n",
    "            prediction_final = 0 if predictions/self.n_estimators<=0.5 else 1\n",
    "            return prediction_final\n",
    "        \n",
    "        else:\n",
    "            predictions = []\n",
    "            for i in self.trees:\n",
    "                predictions_value = i.predict_single(x)\n",
    "                predictions.append(predictions_value)\n",
    "            return np.mean(predictions)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return [self.predict_single(row) for row in x.tolist()] \n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8fcda-d500-4d45-94b2-25d8eb67473d",
   "metadata": {},
   "source": [
    "### test out the above classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c7dc2-1c58-4c16-a8bc-30445e691d14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [2, 3,2,3],\n",
    "    [1, 5,1,5],\n",
    "    [8, 7,8,7],\n",
    "    [9, 6,9,6],\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "tree = Node()\n",
    "tree.fit(X, y, max_depth=3, task_type='class', criterion='gini')\n",
    "\n",
    "preds = tree.predict(X)\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef14e80-65cc-4cdd-9888-d1a641ba1e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RandomF = BaggedTrees().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f743c-7fea-4906-a787-fe32cf76e306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = tree.predict(X)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884975bb-39fc-4419-84c8-6af6a44a9c1e",
   "metadata": {},
   "source": [
    "### Adaboost \n",
    "Weighted averages of the trees instaed of averages as we have taken above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b9f01-b02b-491b-b379-fa51dbd4a560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaBoostClassifier:\n",
    "    def __init__(self, n_estimators=50, max_depth=1, min_samples_split = 2, task_type = 'class', criterion = 'gini', max_features = 3, random_state = 3,learning_rate = 1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.task_type = task_type\n",
    "        self.criterion = criterion\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alphas = []    # list to store alpha_m\n",
    "        self.models = []    # list to store weak learners (your Node)\n",
    "    def fit(self, X , y):\n",
    "        \n",
    "        K = np.unique(y)\n",
    "        num_class = len(K)\n",
    "        \n",
    "        if self.task_type == 'class' and num_class >2 :\n",
    "             self.task_type = 'multiclass'\n",
    "        \n",
    "        \n",
    "        if self.task_type == 'class':\n",
    "            y = np.where(y == 0, -1, 1)\n",
    "        \n",
    "        else:\n",
    "            y = y\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        weights = np.ones(n_samples) / n_samples  # uniform distribution'\n",
    "        epsilon = 1e-10\n",
    "        \n",
    "        for i in range(0,self.n_estimators):\n",
    "            np.random.seed(self.random_state + i)\n",
    "            weights = weights/weights.sum()  # normalize\n",
    "            indices = np.random.choice(len(X), size=len(X), replace=True, p = weights)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            if self.max_features > X_sample.shape[1]:\n",
    "                max_features = X_sample.shape[1]\n",
    "            else : \n",
    "                max_features = self.max_features\n",
    "\n",
    "            learner = Node()\n",
    "            learner.fit(X_sample,y_sample, depth = 0 ,max_depth = self.max_depth, min_samples_split = self.min_samples_split, task_type = self.task_type, criterion = self.criterion, max_features = max_features )\n",
    "\n",
    "\n",
    "            predictions = learner.predict(X_sample)\n",
    "            \n",
    "            if self.task_type == 'class':\n",
    "                predictions = np.where(predictions == 0, -1, 1)\n",
    "                weighted_error = np.sum(weights* (predictions!=y_sample))\n",
    "                if weighted_error <= 0:\n",
    "                    alpha_m = 1e10  # Very strong learner\n",
    "                    self.models.append(learner)\n",
    "                    self.alphas.append(alpha_m)\n",
    "                    break  # Stop adding more learners\n",
    "                alpha_m = 0.5*(np.log((1-weighted_error+epsilon)/(weighted_error+epsilon)))\n",
    "                alpha_m = self.learning_rate * alpha_m\n",
    "                weights = weights * np.exp(-alpha_m * y_sample * predictions)\n",
    "                \n",
    "            elif self.task_type == 'multiclass':\n",
    "                # alpha_m = defaultdict(int)\n",
    "                # predictions = np.where(y == k, 1,0)\n",
    "                weighted_error = np.sum(weights* (predictions!=y_sample))\n",
    "                if weighted_error <= 0:\n",
    "                    alpha_m = 1e10\n",
    "                    self.models.append(learner)\n",
    "                    self.alphas.append(alpha_m)\n",
    "                    break  # Stop adding more learners\n",
    "                # for i in rane(0,K):\n",
    "                alpha_m = np.log((1-weighted_error+epsilon)/(weighted_error+epsilon))+np.log(num_class-1)\n",
    "                alpha_m = self.learning_rate * alpha_m\n",
    "                weights *= np.exp(-alpha_m* (predictions!=y_sample))\n",
    "                \n",
    "                    \n",
    "            else : \n",
    "                \n",
    "                error = np.abs(y_sample - predictions)\n",
    "                normalised_error = error/np.max(error)\n",
    "                normalised_error = np.clip(normalised_error, 1e-10, 1 - 1e-10)\n",
    "                weighted_error = np.sum(weights * normalised_error)\n",
    "                if weighted_error <= 0:\n",
    "                    alpha_m = 1e10  \n",
    "                    self.models.append(learner)\n",
    "                    self.alphas.append(alpha_m)\n",
    "                    break  # Stop adding more learners\n",
    "                beta = weighted_error/(1-weighted_error)\n",
    "                alpha_m = np.log(1/beta)\n",
    "                alpha_m = self.learning_rate * alpha_m\n",
    "                weights = weights * beta**(1-normalised_error)\n",
    "                \n",
    "            self.models.append(learner)\n",
    "            self.alphas.append(alpha_m)\n",
    "                \n",
    "\n",
    "            \n",
    "    def predict_single(self,x):\n",
    "        weighted_prediction = 0 \n",
    "        function_k = defaultdict(int)\n",
    "        sum_alphas = 0\n",
    "        for i,j in zip(self.alphas,self.models):\n",
    "            prediction = j.predict_single(x)\n",
    "            if self.task_type == 'class':\n",
    "                weighted_prediction += (i*prediction)\n",
    "            elif self.task_type == 'multiclass':\n",
    "                function_k[prediction] += i\n",
    "            else:\n",
    "                weighted_prediction += (i*prediction)\n",
    "                sum_alphas +=i\n",
    "        \n",
    "        if self.task_type == 'class':\n",
    "            final_prediction = 1 if weighted_prediction>0 else -1\n",
    "        elif self.task_type == 'multiclass':\n",
    "            final_prediction = max(function_k, key=function_k.get)\n",
    "        else:\n",
    "            final_prediction = weighted_prediction/sum_alphas\n",
    "\n",
    "        \n",
    "        return final_prediction\n",
    "\n",
    "    def predict(self, x):\n",
    "        return [self.predict_single(row) for row in x.tolist()]    \n",
    "    \n",
    "    def feature_importances_(self):\n",
    "        feature_importance = defaultdict(float)\n",
    "        for model, alpha in zip(self.models, self.alphas):\n",
    "            if model.feature_idx is not None:\n",
    "                feature_importance[model.feature_idx] += alpha * model.impurity_decrease\n",
    "        return feature_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7359a3-2521-4bd2-9eb2-bf9f453d794a",
   "metadata": {},
   "source": [
    "### gradiant boosting\n",
    "simultanoulsy work on tress, predict errors and minimise it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb788a-1672-4b4d-bdba-51a9abef432d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def hessian(self, y_true, y_pred):\n",
    "        raise NotImplementedError\n",
    "class MSELoss(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return 0.5 * np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        return -(y_true - y_pred)\n",
    "\n",
    "    def hessian(self, y_true, y_pred):\n",
    "        return np.ones_like(y_true)\n",
    "\n",
    "class GradientBoostingRegressor():\n",
    "    def __init__(self, n_estimators=50, max_depth=1, min_samples_split = 2, task_type = 'gradient', criterion = 'gini', max_features = 3, random_state = 3,learning_rate = 1, loss = None,init_pred = None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.task_type = task_type\n",
    "        self.criterion = criterion\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        self.loss = loss\n",
    "        self.init_pred = init_pred\n",
    "    def fit(self,X,y, lambda_=1.0, gamma=0.0):\n",
    "        self.init_pred = np.mean(y)\n",
    "        y_pred = np.full_like(y, np.mean(y))\n",
    "        self.loss =  MSELoss()\n",
    "        for i in range(0,self.n_estimators ):\n",
    "           \n",
    "            \n",
    "            if self.max_features > X.shape[1]:\n",
    "                max_features = X.shape[1]\n",
    "            else : \n",
    "                max_features = self.max_features\n",
    "\n",
    "            tree = Node()\n",
    "            tree.fit(X,y, depth = 0 ,max_depth = self.max_depth, min_samples_split = self.min_samples_split, task_type = self.task_type, criterion = self.criterion, max_features = max_features,y_pred= y_pred,loss = self.loss,lambda_ = lambda_,gamma = gamma )\n",
    "            self.trees.append(tree)\n",
    "            tree_output = tree.predict(X)\n",
    "\n",
    "          \n",
    "            y_pred += self.learning_rate * np.array(tree.predict(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Step 1: Initialize with array of init_pred\n",
    "        y_pred = np.full(X.shape[0], self.init_pred)\n",
    "\n",
    "        # Step 2: Add tree predictions scaled by learning rate\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * np.array(tree.predict(X))\n",
    "\n",
    "        # Step 3: Return final prediction\n",
    "        return y_pred\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa9366-71ff-4117-ad78-10e76506271c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10.0, random_state=42)\n",
    "model = GradientBoostingRegressor(n_estimators=10, max_depth=3, learning_rate=0.1)\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e80b57-8dd2-4a08-9603-8cbdfc96a629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y, preds)\n",
    "r2 = r2_score(y, preds)\n",
    "\n",
    "print(f\"RMSE: {mse**0.5:.4f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241dfd64-eae7-4540-9e07-58e1dfe07ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
